*** 1- Introduction ***

what we will learn:

- what large language models (LLM) are
- what can we do with them
- how to use them effectively
- practical concepts: tokens, cost, model, selection, etc

*** 2- Rise of AI Engineering ***
---

# Lecture Notes: The Rise of the AI Engineer

## 1. Introduction

Since the release of ChatGPT, the software industry has been evolving rapidly. New models, tools, APIs, expectations, and job roles are emerging at a fast pace. One of the most notable new roles is the **AI Engineer**.

## 2. AI Engineer vs. Machine Learning Engineer

### Machine Learning Engineer

* Focuses on building and training models.
* Works with data cleaning, model architectures, and tuning.
* Involves heavy math, research, and optimization of training pipelines.

### AI Engineer

* Focuses on using **pre-trained models**, especially large language models (LLMs).
* Does not need deep mathematical or theoretical understanding of model internals.
* Needs strong skills in integrating models into real-world applications.
* Similar to how software engineers use databases without knowing internal implementation details.
* Main goal: apply existing AI capabilities to solve practical problems.

## 3. What AI Engineers Build

Companies today are increasingly hiring engineers who can integrate AI-powered features into their products. Examples include:

* Summarization
* Translation
* Intelligent search
* Automation tools
* Personalized user experiences

## 4. Real-World Examples

### 1. Amazon Product Review Summaries

* AI-generated summaries help customers understand reviews quickly.
* Speeds up decision-making and increases conversions.

### 2. ActiveCampaign

* Allows marketers to generate full email campaigns from prompts.
* Eliminates the need to start from a blank page.

### 3. Translation on Social Platforms (e.g., X/Twitter)

* LLMs detect language, identify the user’s locale, and generate translations instantly.
* Now a standard feature across social and news platforms.

### 4. Content Moderation on YouTube and Twitch

* AI automatically flags spam, hate speech, and inappropriate content.
* Reduces the need for large teams of human moderators.

### 5. Freshdesk Ticket Routing

* AI categorizes, prioritizes, and routes user support tickets.
* Helps agents spend more time solving issues instead of sorting them.

### 6. Redfin Real Estate Assistant

* Built-in chat assistant answers property-specific questions instantly.
* Reduces friction and removes the need to wait for human agents.

## 5. Why AI Engineering Matters

* AI features are becoming standard, not optional.
* They save time, cut costs, and improve user experience.
* The demand for developers who can work with LLMs is increasing quickly.
* In the future, AI tooling knowledge will be as essential as database skills are today.

## 6. Core Skills for AI Engineers

* Understanding of large language models (LLMs)
* Prompt engineering
* Retrieval augmented generation (RAG)
* Vector databases
* Building AI agents
* Integrating AI with real-world applications

## 7. Course Direction

This course introduces the foundations needed to work effectively with modern AI systems.
The next lesson will explain what a large language model is in simple, practical terms.

---

*** 3- What are Large Language Models ***
---

# Lecture Notes: What Is a Large Language Model (LLM)

## 1. Introduction

Now that the importance of AI is clear, this lesson explains a core concept: the **large language model**.

## 2. What Is a Language Model

* A language model is a system trained to understand and generate human language.
* It can process text, detect patterns, and generate coherent output.

## 3. Examples of Language Models

### Commercial Models

* GPT by OpenAI
* Gemini by Google
* Claude by Anthropic
* Grok by xAI

### Open Source Models

* Llama by Meta
* Mistral by Mistral AI
* Many others available publicly.

## 4. Why They Are Called “Large”

* Trained on massive datasets: books, articles, forums, code, documentation, and more.
* Contain billions of parameters.
* These parameters encode patterns in grammar, structure, tone, and factual associations.

## 5. How LLMs Work Conceptually

* When you prompt an LLM with a sentence like “The capital of France is”, it does not search for the answer.
* It predicts the most likely next words based on patterns learned during training.
* This is similar to autocomplete, but far more powerful.

## 6. Nature of an LLM

* A large language model is essentially a huge mathematical structure.
* The model does not understand language the way humans do.
* It does not possess beliefs or intelligence.
* It generates outputs based on probability, not truth.

### Why Answers Vary

* Asking the same question multiple times produces different responses.
* This happens because the model generates new text each time rather than retrieving stored answers.

## 7. The Importance of Training Data

* Training data determines the model’s behavior.
* If the data is biased, low quality, or inaccurate, the model’s output will reflect that.
* This explains:

  * Political bias in some models
  * Confident but incorrect answers
  * Inconsistent reasoning

## 8. Models Generating Code

* LLMs are trained on billions of lines of code from public repositories.
* Much of this code is outdated, poorly designed, or incorrect.
* The model cannot distinguish good code from bad code.
* As a result:

  * Generated code may look correct
  * It may compile
  * But it can still be insecure, buggy, or full of anti-patterns
* Blind trust in model-generated code is risky.

## 9. Garbage In, Garbage Out

* A model’s quality mirrors the quality of its training data.
* Clean, accurate data produces reliable models.
* Messy or biased data produces unreliable results.

## 10. Training Costs and Constraints

* Training large models requires:

  * Massive datasets
  * Thousands of GPUs
  * Weeks or months of computation
  * Extremely expensive infrastructure
* Only a small number of companies can afford to train such models from scratch.

## 11. Role of Developers vs. ML Engineers

* Developers typically do not train models.
* Instead, developers:

  * Use pre-trained models
  * Write effective prompts
  * Understand model limitations
  * Integrate models into applications
* This is similar to using a database without building one yourself.

## 12. Course Direction

The upcoming lessons will cover important foundational concepts such as:

* Tokens
* Cost
* How to choose the right model for a task

---

*** 4- What Can You Do With Language Models ***
---

# Lecture Notes: How Developers Integrate LLMs into Applications

## 1. Position of LLMs in Application Architecture

Modern applications typically include:

* A frontend (often built with React or similar).
* A backend server.
* A database for storing application data.
* A language model (LLM) used to generate or process content.

The LLM is not the core of the application. It acts as a supporting system.

* We send the model input (a prompt).
* We receive a response.
* The application uses that response to improve or automate user experiences.

## 2. Common Use Cases for LLM Integration

### 1. Summarization

* Used to create concise summaries of long text.
* Example: Amazon summarizes product reviews on product pages.
* Now a standard feature in many modern applications.

### 2. Content Generation

* Generating emails, product descriptions, advertisements, and social media posts.
* Saves time and removes the need to create content from scratch.

### 3. Text Classification

* Categorizing user input automatically.
* Example questions models can answer:

  * Is this message spam or not?
  * Is this review positive or negative?
  * What category does this support ticket belong to (billing, login, cancellation)?

LLMs can output structured data such as JSON, allowing the backend to:

* Parse the result
* Store it
* Make automated decisions

### 4. Translation

* Converting text from one language to another.
* Used in platforms like X/Twitter.
* Built into modern systems such as iOS for real-time translation.

### 5. Information Extraction

* Pulling structured data from unstructured sources.
* Example: Extracting invoice details (invoice number, amount, names, addresses) from messy text or PDFs.

### 6. Chatbots

* Creating conversational assistants.
* Can answer questions using:

  * User-specific data
  * Business documents
  * Knowledge bases
* Often integrated directly into websites or applications.

## 3. A Common Pattern: Text In → Text Out

All of these use cases follow a consistent workflow:

1. The application sends a prompt or text input.
2. The LLM returns text or structured output.

The output can be:

* Plain text
* Arrays
* JSON objects
* Numbers
* Images
* Or any form of content that the application can use

## 4. Next Steps

Now that the use cases are clear, the next lesson will explore what is actually inside a large language model and how it works internally.

---


*** 5- Understanding Tokens and Context Window ***
---

# Lecture Notes: Understanding Tokens in Language Models

## 1. What Are Tokens

* When we send text to a language model, it does not process the text as plain sentences.
* Instead, it breaks the input into **tokens**, which are smaller units.
* A token can be:

  * A whole word
  * Part of a word
  * Punctuation
  * Spaces
  * Emojis
* Tokens are not equal to characters or words; they fall somewhere in between.

## 2. Visualizing Tokens

* The OpenAI tokenizer tool shows how text is split into tokens.
* Example:

  * A sample text with 252 characters may become 53 tokens.
* The tool color-codes each token so you can see how the model splits the text.

## 3. Why Tokens Matter

Tokens impact two major factors:

### 1. **Cost**

* Billing is based on the number of tokens processed.
* Example pricing (at the time of the lecture):

  * GPT-4.0 mini: 1 million output tokens cost 60 cents.
  * GPT-4.1: 1 million output tokens cost 8 dollars.
* The difference can be significant, especially for tasks involving:

  * Long documents
  * Large summaries
  * High-volume content generation
* Choosing a model should be based on application needs, not just using the newest or most powerful one.

### 2. **Context Window Limit**

* Each model has a maximum number of tokens it can handle at once.
* This includes:

  * The prompt (input)
  * The output
  * The conversation history (if chat-based)
* Examples of context window sizes:

  * GPT-4.0 mini: about 128,000 tokens
  * GPT-4.1: about 1,000,000 tokens
  * Mistral (open source): about 32,000 tokens

### What Happens When You Exceed It

* If your input + output exceed the context window,
  the model stops generating, sometimes mid-sentence.
* Knowing the context limit helps you avoid incomplete outputs.

## 4. Choosing the Right Model

* You do not always need the largest or most expensive model.
* For tasks like:

  * Summarizing a blog post
  * Categorizing a support ticket
  * Basic text generation
    smaller or mid-range models (like Mistral or GPT-4.0 mini) are often enough.
* The choice should always reflect the actual needs of your application.

## 5. Next Lesson

The next lesson will show how to **count tokens programmatically**, which helps estimate cost and ensure requests stay within the model’s token limits.

---

*** 6 - Counting Tokens ***
---

# Lecture Notes: Counting Tokens Programmatically

## 1. Project Setup

### Create a Playground Directory

1. Open a terminal.
2. Navigate to your desktop or any preferred location.
3. Create a new project directory:

   * `mkdir playground`
4. Move into the directory:

   * `cd playground`

### Initialize a Node Project

* Run:
  `npm init -y`
* This generates a default `package.json` without asking configuration questions.

## 2. Installing the Tokenizer Library

### Install tiktoken

* Run:
  `npm install tiktoken`
* Note:
  Different AI platforms use different tokenizer libraries.
  `tiktoken` is used for OpenAI models.

## 3. Open the Project in VS Code

* Run:
  `code .`
* If the command does not work, drag the folder into VS Code manually.

## 4. Writing the Token Counting Script

### Create File

* Add a new file: `index.js`

### Import the Tokenizer

-------------------code----------------------
import { get_encoding } from "tiktoken";
-------------------code----------------------

### Choose an Encoding

* Call `get_encoding` with an encoding type.
* Example:

  * `cl100k_base`
  * This stands for “chat language” with a dictionary of about 100,000 tokens.

### Encode Text

-------------------code----------------------
const encoding = get_encoding("cl100k_base");

const tokens = encoding.encode(
  "hello world, this is the first test of tiktoken library."
);

console.log(tokens);
-------------------code----------------------

## 5. Running the Script

### First Error Encountered

* Running `node index.js` produces this error:

  * `SyntaxError: Cannot use import statement outside a module`
* Reason:
  Node interprets JavaScript files as **CommonJS modules** by default, but the script uses **ES module syntax**.

### Fixing the Error

* Open `package.json`.
* Add the following property:

  -------------------code----------------------
  "type": "module"
  -------------------code----------------------
* This tells Node to treat the project as an ES module environment.

### Run Again

* Run: `node index.js`
* Output:

  * An array of numbers, e.g., 13 items
  * Each number represents a **token ID** that maps to an actual token in the model’s vocabulary.

## 6. Why This Matters

* When handling large text inputs, counting tokens beforehand helps:

  * Estimate cost
  * Avoid exceeding the model’s context window
  * Control prompt size programmatically
* `tiktoken` provides a simple way to calculate token usage before sending requests to an LLM.

---

- node index6.js

The output:
-------------------code----------------------
Uint32Array(13) [
  15339, 1917,   11,
    420,  374,  279,
   1176, 1296,  315,
  87272, 5963, 6875,
     13
]
-------------------code----------------------

means **the text was tokenized into 13 tokens**.

* Each number in the array is a **token ID**.
* These IDs correspond to specific pieces of text in the model’s vocabulary (words, parts of words, punctuation, etc.).
* So even though your text had more than 13 characters or words, the tokenizer split it into **13 meaningful tokens** for the LLM.

In short: **your text = 13 tokens**.


Now You can map the token IDs back to readable text using the same `tiktoken` library. Here’s how you can do it:

---

### **Step 1: Get the token strings**

`tiktoken` provides a `.decode()` method that converts token IDs back into text.

### **Example `index6.js`**

-------------------code----------------------
import { get_encoding } from "tiktoken";

// Initialize the tokenizer
const encoding = get_encoding("cl100k_base");

// Your text
const text = "hello world, this is the first test of tiktoken library.";

// Encode the text into token IDs
const tokenIds = encoding.encode(text);

// To track positions in original text
let currentIndex = 0;

console.log("Token breakdown:");

// Decode each token individually into string
tokenIds.forEach((id, idx) => {
  const tokenBytes = encoding.decode([id]); // returns Uint8Array
  const tokenText = new TextDecoder().decode(tokenBytes); // convert to string

  // Find start and end in original text
  const start = text.indexOf(tokenText, currentIndex);
  const end = start + tokenText.length;
  currentIndex = end;

  console.log(
    `Token ${
      idx + 1
    }: ID=${id}, Text="${tokenText}", Start=${start}, End=${end}`
  );
});

// Decode full text back to string
const fullTextBytes = encoding.decode(tokenIds);
const decodedText = new TextDecoder().decode(fullTextBytes);

console.log("\nDecoded full text:", decodedText);
-------------------code----------------------

---

### **Step 2: Run the script**

-------------------code----------------------
node index6.js
-------------------code----------------------

---

### **What you’ll get**

1. **Token IDs** – array of numbers representing each token.
2. **Tokens** – array of strings showing what each token represents individually.
3. **Decoded full text** – reconstructs the original text from tokens.

---

This way you can **see exactly how the model is splitting your input** and understand why it counted 13 tokens.



*** 7- Choosing the Right Model ***

---

## **Choosing the Right AI Model**

### **1. No Single Best Model**

* The choice depends entirely on the application requirements.
* Focus on criteria rather than fixed model names because names change frequently.

---

### **2. Key Factors to Consider**

#### **a Model Intelligence**

* Complex problems → stronger reasoning models.
* Simple tasks (text extraction, short summaries, classification) → smaller models are sufficient.

#### **b Response Speed**

* Larger models are slower, especially for long outputs.
* Real-time features (autocomplete, quick summaries) require faster models.

#### **c Input and Output Types**

* Most common: text.
* Multimodal models (LMMs) handle text, images, audio, video, or combinations.
* Example: If your app needs image description, you need a multimodal model.

#### **d Cost**

* Cost is based on the number of tokens.
* Long documents or high-volume generation can be expensive.
* Always compare costs between models before choosing.

#### **e Context Window**

* Maximum text the model can process at once (input + response + chat history).
* Longer documents, code analysis, or extended conversations require larger context windows.

#### **f Privacy**

* Sensitive data (medical records, personal info) may require self-hosted or open-source models.
* Avoid sending sensitive data to external servers if privacy is a concern.

---

### **3. Practical Example: OpenAI Models**

* Example models: GPT-4.1, GPT-4.0 Mini, GPT-3.
* **Comparison:**

  * GPT-3: strongest reasoning, slowest.
  * Smaller models: sufficient for summarization or structured data extraction.
* Multimodal support:

  * Most models: input text + images, output only text.
  * GPT Image1: input text + images, output images only.

---

### **4. Cost Considerations**

* Input and output tokens are priced differently.
* High-volume processing (e.g., contracts) can be expensive.
* Always choose a model that balances performance and cost.

---

### **5. Token Limits and Context**

* Context window: maximum tokens a model can process at once.
* Max output tokens: maximum tokens in a single response.

  * Example: A model may have a large context window but limit response to 32,000 tokens.
* Knowledge cutoff: indicates when the model’s training data ends.

  * Older models may lack up-to-date knowledge but can still work for specific tasks.

---

### **6. Summary**

* No “one-size-fits-all” model.
* Choose based on:

  1. Required intelligence and reasoning.
  2. Speed and responsiveness.
  3. Supported input/output types.
  4. Cost and token usage.
  5. Context window needs.
  6. Privacy requirements.
* Compare models and pick the one that fits your application’s needs.

---

Next step: Learn about **model settings** and how parameters affect the output.

---
