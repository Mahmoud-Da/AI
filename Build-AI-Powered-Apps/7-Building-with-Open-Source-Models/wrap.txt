*** 1- Introduction ***
**Title: Using Open Source Models in Applications**

---

## 1. Overview of This Section

This section introduces **open source AI models** and explains their role in modern software applications. The focus is on understanding why and when to choose open source models, how to find them, how to run them locally, and how to integrate them into an application to power AI features **without relying on hosted or third-party services**.

---

## 2. What Is Being Built

* An application that uses **open source AI models** as its intelligence layer.
* The AI features are powered **locally** rather than through cloud-based or hosted AI services.
* The models will be:

  * Discovered and selected from open source ecosystems
  * Run locally using tooling such as **Ollama**
  * Integrated directly into the application architecture

---

## 3. Why Open Source Models Are Used

Open source models are introduced as an alternative to hosted AI services.

Key reasons for choosing open source models include:

* **Independence from hosted services**

  * No reliance on third-party APIs or external providers
  * Reduced risk of service outages or API changes
* **Local execution**

  * Models can run entirely on the developer‚Äôs machine or server
  * Suitable for offline or restricted environments
* **Control and flexibility**

  * Full control over model versions, updates, and configuration
  * Easier customization and experimentation
* **Cost considerations**

  * Avoids per-request or per-token pricing common with hosted AI services

---

## 4. Problems This Approach Solves

Using open source models addresses several common issues:

* Dependency on external AI providers
* Network latency caused by remote API calls
* Privacy concerns related to sending data to third-party services
* Long-term cost scaling issues as AI usage increases

By running models locally, the application can deliver AI functionality while remaining self-contained.

---

## 5. How Open Source Models Are Found

This section indicates that developers will learn:

* Where to **discover open source models**
* How to evaluate which models are suitable for their use case

While specific repositories or platforms are not yet detailed, the emphasis is on understanding that open source models are widely available and selectable based on application needs.

---

## 6. Running Models Locally

### 6.1 Local Execution Concept

* Open source models can be executed directly on a local machine.
* This removes the need for cloud-based inference.

### 6.2 Tooling: Ollama

* **Ollama** is introduced as a tool used to run open source models locally.
* It acts as a local model runtime and management layer.
* It simplifies:

  * Downloading models
  * Running inference locally
  * Managing model versions

The use of Ollama enables developers to treat local models similarly to a service, but without external dependencies.

---

## 7. Application Integration

After running models locally, the next step is integration.

Key points of integration:

* The locally running open source models will be connected to the application.
* These models will **power AI features** inside the app.
* The application will communicate with the local model runtime instead of a hosted AI API.

This creates an architecture where AI capabilities are fully embedded within the application‚Äôs own infrastructure.

---

## 8. Architecture Implications

### Backend Logic

* The backend is responsible for interacting with the locally running model.
* AI requests are processed internally rather than forwarded to external services.

### API Interaction

* The application communicates with the local model runtime (e.g., via a local API or process).
* This replaces calls to remote hosted AI providers.

### Performance and UX Considerations

* Local execution can reduce latency.
* Performance depends on local hardware resources.
* The user experience benefits from faster, more predictable AI responses.

---

## 9. Summary of the Section Flow

* Introduce open source AI models
* Explain why they are a viable and often preferable choice
* Show how to find appropriate models
* Demonstrate how to run them locally using tools like Ollama
* Integrate them into an application to enable AI features without hosted dependencies

This section sets the foundation for building AI-powered applications that are self-hosted, controllable, and independent.

---

*** 2- Why Use Open-Source Models ***
## Optional Further Study Suggestions

* If you want to go deeper, you may study how different open source models compare in terms of size, accuracy, and hardware requirements.
* An optional improvement would be to explore deployment strategies for running local models in production environments.
* You may also study security and resource isolation considerations when running AI models locally within an application stack.

---

*** 2- Why Use Open-Source Models ***
**Title: Why Choose Open Source AI Models Over Hosted Services**

---

## 1. Purpose of This Lesson

This lesson explains **why open source AI models are worth using** instead of relying exclusively on hosted providers such as OpenAI or Anthropic. It provides concrete, practical reasons grounded in cost, privacy, flexibility, deployment constraints, and transparency.

The goal is to help developers make an informed architectural decision when choosing the AI layer of their application.

---

## 2. Problem Context: Hosted Models vs Open Source Models

Hosted AI models (for example, OpenAI or Anthropic):

* Are accessed via remote APIs
* Require a continuous internet connection
* Charge based on usage, typically per token
* Operate as black-box systems with limited visibility into internals

Open source models offer an alternative approach by allowing developers to **run models themselves**, either locally or on their own infrastructure.

---

## 3. Reason 1: Cost Control

### Problem with Hosted Models

* Hosted models charge **per token**
* As application usage grows, costs can increase rapidly and unpredictably
* High-volume or long-running applications can become expensive over time

### How Open Source Solves This

* Open source models can be run:

  * Locally on a developer machine
  * On a self-managed server
* After the initial hardware setup:

  * There is no per-token cost
  * Usage does not directly increase billing
* Developers have full control over infrastructure spending

**Key Outcome:** Costs become predictable and controllable.

---

## 4. Reason 2: Privacy and Data Security

### Problem with Hosted Models

* Prompts and responses are sent outside the application environment
* Sensitive data is transmitted to third-party servers

### How Open Source Solves This

* Models run entirely within the developer‚Äôs environment
* No data leaves the local machine or private server
* Prompts and outputs remain internal

### Why This Matters

This is critical for applications handling sensitive data, such as:

* Healthcare systems
* Financial applications
* Government or regulated projects

**Key Outcome:** Full data privacy and compliance control.

---

## 5. Reason 3: Flexibility and Model Choice

### Limitations of Hosted Models

* Developers are locked into a small set of provider-approved models
* Switching models often requires API changes or vendor migration

### How Open Source Solves This

* Access to **hundreds or thousands of models**
* Freedom to:

  * Switch models easily
  * Experiment with different architectures
  * Choose models optimized for specific tasks

### Small Language Models (SLMs)

* Many open source models are **small language models**
* These models are:

  * Tuned for narrow or specific tasks
  * More efficient than large general-purpose models
* For certain use cases, they may:

  * Perform better than large commercial LLMs
  * Require fewer resources

**Key Outcome:** Model selection is task-driven, not vendor-driven.

---

## 6. Reason 4: Offline Access

### Problem with Hosted Models

* Require a stable internet connection
* Cannot function offline

### How Open Source Solves This

* Models can run completely offline
* No network dependency once installed

### Use Cases

* Edge devices
* Fieldwork environments
* Remote or low-connectivity locations

**Key Outcome:** AI functionality is available anywhere, regardless of connectivity.

---

## 7. Reason 5: Transparency

### Limitations of Hosted Models

* Internal implementation is hidden
* Training data and model behavior are opaque
* Biases are difficult to inspect or verify

### How Open Source Solves This

* Developers can:

  * Inspect the model code
  * Review architecture details
  * Sometimes examine training data sources
* This allows better understanding of:

  * Model behavior
  * Limitations
  * Potential biases

**Key Outcome:** Increased trust, auditability, and explainability.

---

## 8. Summary of Benefits

Open source AI models provide:

* Cost savings
* Strong privacy guarantees
* Flexibility in model selection
* Offline execution capabilities
* Transparency into how models work

These advantages make open source models a strong choice for many production and research use cases.

---

## 9. Transition to the Next Lesson

The next lesson will focus on **how to find open source models**, building on the motivation and benefits explained here.

---

## Optional Further Study Suggestions

* If you want to go deeper, you may study cost modeling comparisons between hosted AI services and self-hosted open source models.
* An optional improvement would be to explore criteria for choosing between small language models and large language models.
* You may also study regulatory and compliance requirements that benefit from on-premise or offline AI execution.


*** 3- Finding Open-Source Models ***
**Title: Discovering and Evaluating Open Source Models with Hugging Face**

---

## 1. Purpose of This Lesson

This lesson explains **how to find open source AI models** using **Hugging Face**, which is the primary platform for discovering, evaluating, and experimenting with open source machine learning models. The focus is on understanding the platform structure, model categories, evaluation signals, and initial ways to try models before integrating them into an application.

---

## 2. What Hugging Face Is and Why It Is Used

### 2.1 What Hugging Face Provides

* Hugging Face is the **go-to platform** for hosting and discovering open source models.
* It serves a similar role to **GitHub for machine learning**.
* Developers and researchers publish:

  * Open source models
  * Datasets
  * Interactive demos (Spaces)

### 2.2 Why Hugging Face Is Important

* Centralized ecosystem for open source AI
* Standardized model documentation
* Built-in tools for testing and integration
* Supports multiple programming languages and workflows

---

## 3. Navigating the Hugging Face Platform

### 3.1 Main Sections

On the Hugging Face website, the top navigation includes:

* Models
* Datasets
* Spaces
* Other supporting resources

This lesson focuses specifically on the **Models** section.

---

## 4. Finding Models

### 4.1 Searching by Model Name

* If you already know the model name, you can search directly using the search bar.
* Example: searching for a model such as *Mistral*.

### 4.2 Browsing Using Filters

If you do not know a specific model, filters on the left side allow discovery by task and category.

---

## 5. Model Categories and Tasks

### 5.1 Task-Based Filtering

Hugging Face organizes models by **tasks**, such as:

* Text generation
* Image-to-text
* Image-to-image

These tasks can be expanded to reveal more options.

### 5.2 High-Level Categories

Expanded categories include:

#### Multimodal

* Models that support multiple data types simultaneously
* Examples: text and image together

#### Computer Vision

* Models focused on images and videos

#### Natural Language Processing (NLP)

* Models focused on text-based tasks
* Includes tasks such as:

  * Text classification
  * Translation
  * Summarization

---

## 6. Example: Selecting a Summarization Model

### 6.1 Filtering by Task

* The summarization task is selected under NLP.
* Approximately **2400 models** are available for text summarization.

### 6.2 Sorting Models

Models can be sorted by:

* Trending
* Number of likes
* Number of downloads
* Other popularity signals

The lesson demonstrates sorting by **number of downloads** to identify widely used models.

---

## 7. Understanding Model Listings

Each model listing displays key evaluation information:

* Publisher (for example, Facebook)
* Model name (e.g., BART CNN)
* Intended task (summarization)
* Number of parameters

  * Represents model size
  * More parameters generally means a larger and more capable model
  * Compared to a larger ‚Äúbrain‚Äù
* Last updated date
* Number of downloads
* Number of likes

These signals help determine model maturity, popularity, and maintenance status.

---

## 8. Exploring a Model in Detail

### 8.1 Model Card

The model card provides:

* Description of what the model does
* Model architecture
* Training details
* Intended use cases
* Limitations and notes

This is the primary documentation for understanding a model.

### 8.2 Files Tab

* Represents the underlying **Git repository**
* Shows:

  * Branches
  * Commit history
  * Model files and configuration
* Confirms that the model is fully open source and version-controlled

---

## 9. Trying a Model in the Browser

### 9.1 Playground

* Hugging Face provides an in-browser playground directly on the model page.
* Allows users to:

  * Input custom text
  * Use built-in examples
  * See model output in real time

### 9.2 Authentication Requirement

* Using the playground requires:

  * Signing up
  * Logging in
* This process is free and quick.

### 9.3 Example Execution

* Text is submitted for summarization.
* The model processes the input.
* The response is returned in approximately **2 seconds**.

---

## 10. Viewing Code Examples

### 10.1 View Code Feature

* Hugging Face provides example code for:

  * Python
  * JavaScript
  * Curl (command-line)

These examples show how to run the model programmatically.

---

## 11. Using Curl

* Curl examples demonstrate:

  * Sending an HTTP POST request
  * Targeting the Hugging Face inference endpoint
  * Passing input text
  * Receiving summarized output

This shows that the model can be accessed via a standard HTTP API.

---

## 12. Using JavaScript

There are **two JavaScript integration approaches**.

### 12.1 Direct HTTP Requests (Fetch or Axios)

* Use `fetch` or `axios` to send a POST request
* Request includes:

  * Authorization header
  * Hugging Face access token
* The token must be obtained from Hugging Face (covered in the next lesson)
* The response contains the summarized output

This approach exposes the underlying HTTP communication directly.

---

### 12.2 Using `huggingface.js`

* `huggingface.js` is a JavaScript library provided by Hugging Face
* It abstracts away low-level HTTP requests

#### How It Works

* Import the inference client
* Create an inference client instance
* Call high-level utility methods (e.g., `summarization`)
* Provide:

  * Model name
  * Input text
  * Provider

This approach works at a higher level of abstraction and simplifies usage.

---

## 13. Important Privacy Consideration

Regardless of whether you use:

* Curl
* Fetch or Axios
* `huggingface.js`

**All prompts and responses are sent over the network** to Hugging Face servers.

### Implication

* This approach does **not** provide full privacy
* Data leaves your environment

### Conclusion

* If privacy is critical, this is **not** the correct way to use open source models
* An alternative approach will be covered later in this section

---

## 14. Transition to the Next Lesson

The next lesson will demonstrate:

* How to use `huggingface.js`
* How to integrate an open source model into an application

This builds on the discovery and evaluation steps covered here.

---

## Optional Further Study Suggestions

* If you want to go deeper, you may study how to interpret model cards to assess risks, biases, and limitations.
* An optional improvement would be to compare model parameter sizes against hardware constraints.
* You may also study the trade-offs between hosted inference APIs and fully local execution of open source models.

*** 4- Calling Hugging Face Models ***
**Title: Calling Hugging Face Models Using Access Tokens and Integrating Them into the Backend**

---

## 1. Purpose of This Lesson

This lesson explains **how to call Hugging Face open source models programmatically** and integrate them into an existing backend application. It covers:

* Creating and managing Hugging Face access tokens
* Installing and using the Hugging Face inference client
* Extending an existing LLM abstraction without breaking existing functionality
* Wiring a Hugging Face summarization model into the application
* Testing the integration
* Understanding why model choice matters based on training data

---

## 2. Creating a Hugging Face Access Token

### 2.1 Why an Access Token Is Required

* Hugging Face models hosted via inference require authentication
* The access token authorizes API requests to Hugging Face services

---

### 2.2 Steps to Create an Access Token

1. Go to the Hugging Face website
2. Navigate to:

   * Profile
   * Settings
   * Access Tokens
3. Create a new access token

---

### 2.3 Token Management Best Practices

* Always create **separate tokens** for:

  * Development
  * Testing
  * Production
* Reason:

  * If a token is compromised, it limits the blast radius
  * Prevents attackers from abusing all environments

---

### 2.4 Token Configuration

* Give the token a **descriptive name**

  * Example: `office-mac`
  * Indicates it is used on a development machine in the office
* Select permissions:

  * Use the **Read** tab
  * Grant read-only access to resources
* Create the token
* Copy the token immediately

---

## 3. Storing the Token in the Application

### 3.1 Environment Configuration

* Open the `.env` file
* Add a new environment variable:

  * Key: `HF_TOKEN`
  * Value: the newly created Hugging Face access token
-------------------code----------------------
HF_TOKEN="your-huggingFace-token"
-------------------code----------------------
This keeps secrets out of source code and enables environment-specific configuration.

---

## 4. Installing the Hugging Face Inference Client

### 4.1 Inference Service Overview

* Hugging Face provides a hosted inference service
* It includes:

  * A **free tier** (used in this course)
  * Usage limitations:

    * Rate limiting
    * Lower performance
* Suitable for development, not ideal for production

From the server directory, install the Hugging Face inference client:
-------------------code----------------------
bun add @huggingface/inference
-------------------code----------------------

---

### 4.2 Production Considerations

* For production use:

  * Hugging Face offers **dedicated inference endpoints**
* Pricing information:

  * Available on the Hugging Face pricing page
  * Includes different providers and hardware architectures
* Dedicated endpoints provide:

  * Better performance
  * Fewer limitations
* In this course:

  * Only the free tier is used

---

### 4.3 Installing the Client Library

* Open a terminal
* Navigate to the server directory
* Install the Hugging Face inference client package

This library enables programmatic access to Hugging Face-hosted models.

---

## 5. Existing Architecture: LLM Client Abstraction

### 5.1 Current State

* Earlier in the course, an **LLM client abstraction** was introduced
* This abstraction currently uses **OpenAI**
* It centralizes language model calls

---

### 5.2 Design Decision

* OpenAI integration is **not removed**
* Hugging Face support is **added alongside it**
* Reason:

  * Avoid breaking existing features
  * Preserve chatbot functionality
  * Allow multiple LLM providers

---

## 6. Adding Hugging Face Support to the LLM Client

### 6.1 Importing the Inference Client

* Import the inference client from the Hugging Face library
* Add the import at the top of the module
-------------------code----------------------
import { InferenceClient } from "@huggingface/inference";
-------------------code----------------------
---

### 6.2 Creating the Hugging Face Client Instance

* Create a new client object using the access token
* Rename clients to avoid confusion:

  * `openAIClient`
  * `inferenceClient`

This makes the codebase clearer and avoids naming collisions.
-------------------code----------------------
const inferenceClient = new InferenceClient(process.env.HF_TOKEN);
-------------------code----------------------
---

## 7. Using the Hugging Face Summarization Model

### 7.1 Reference Code from the Model Page

* The model used is **BART CNN**
* Hugging Face provides JavaScript examples using `huggingface.js`
* The summarization example includes:

  * Model
  * Inputs
  * Provider
-------------------code----------------------
async summarize(text: string): Promise<string> {
  const output = await inferenceClient.summarization({
    model: "facebook/bart-large-cnn",
    inputs: text,
    provider: "hf-inference",
  });

  return output.summary_text;
},
-------------------code----------------------
---

### 7.2 Avoiding Breaking Changes

* The existing `generateText` method is **not modified**
* Reason:

  * It is used by the chatbot
  * Modifying it could break existing functionality

---

### 7.3 Creating a New Method

* Define a new method named `summarize`
* Input:

  * `text: string`
* Purpose:

  * Summarize review content only
  * Keep responsibilities separated

---

### 7.4 Implementing the Summarize Method

Implementation details:

* Mark the method as `async`
* Use `inferenceClient` instead of the OpenAI client
* Call the `summarization` method
* Provide:

  * `model`: `facebook/bart-large-cnn`
  * `inputs`: the input text
  * `provider`: `hf-inference`

---

### 7.5 Handling the Response

* The inference call returns an output object
* The summary is located at:

  * `output.summary_text`
* The method returns this summary text

---

## 8. Integrating with the Review Service

### 8.1 Updating the Review Summarization Logic

* Navigate to the review service
* In the `summarizeReviews` method:

  * Replace the call to `llmClient.generateText`
  * Use the new `summarize` method instead
-------------------code----------------------
console.log(joinedReviews);
// const { text: summary } = await llmClient.generateText({
//   model: "gpt-5-mini",
//   prompt,
//   maxTokens: 1000,
// });
const summary = await llmClient.summarize(joinedReviews);

await reviewRepository.storeReviewSummary(productId, summary);

return summary;
-------------------code----------------------
---

### 8.2 Simplifying Method Usage

* Previously:

  * An object was passed to the LLM client
  * The result required destructuring
* Now:

  * Pass the joined reviews string directly
  * Receive the summary as a string
  * Store it directly in the database

---

## 9. Testing the Application

### 9.1 Running the Test

* Open the application in the browser
* Trigger the review summarization feature

---

### 9.2 Observed Output

* Example output:

  * ‚ÄúNoise cancellation is top notch and I can wear them during runs or work calls without discomfort.‚Äù
* Observation:

  * This does not look like a true summary
  * It reads like a single user review

---

## 10. Understanding the Unexpected Result

### 10.1 Root Cause Analysis

* Reviewing the model card reveals:

  * The model was pretrained on English
  * Fine-tuned on **CNN / Daily Mail**
* This dataset consists of **news-style articles**

---

### 10.2 Why This Matters

* The model is optimized for:

  * Summarizing news articles
* It is not designed for:

  * Aggregating multiple reviews
  * Extracting key positive and negative themes
  * Product review summarization

---

## 11. Key Lesson Learned

* Not all summarization models are interchangeable
* A model‚Äôs:

  * Training data
  * Fine-tuning objective
* Directly affects its output quality for a given task

---

## 12. Transition to the Next Lesson

The next lesson will focus on:

* **How to choose the right model for the job**
* Matching model training and capabilities to real application requirements

---

## Optional Further Study Suggestions

* If you want to go deeper, you may study how training datasets influence model behavior and output style.
* An optional improvement would be to evaluate review-specific or sentiment-aware summarization models.
* You may also study strategies for benchmarking multiple models against the same task before selecting one for production.


- clear the DB summaries
1- check the tables
-------------------code----------------------
mysql> SHOW TABLES;
+-----------------------------+
| Tables_in_review_summarizer |
+-----------------------------+
| _prisma_migrations          |
| products                    |
| reviews                     |
| summaries                   |
+-----------------------------+
4 rows in set (0.002 sec)
-------------------code----------------------

2- check the data:
-------------------code----------------------
mysql> SELECT * FROM summaries;
+----+-----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------+-------------------------+
| id | productId | content                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | generatedAt             | expiresAt               |
+----+-----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------+-------------------------+
|  1 |         1 | Overall, reviewers praise the iPhone 15 Pro for strong performance (A17 Pro, smooth iOS 17, 120Hz ProMotion), a premium lightweight titanium build and improved camera system (48MP sensor, low‚Äëlight, portrait/focus control, 5x zoom) that often replaces a DSLR for casual use. Key conveniences cited are the new USB‚ÄëC port and useful features like Dynamic Island and satellite emergency connectivity. Common negatives include overheating during heavy use or charging, aggressive image processing/skewed skin tones for some shots, modest battery gains (solid but not game‚Äëchanging), limited Action Button options, and disappointment over minimal design changes, base storage size, and the high price. Overall sentiment is very positive‚Äîmany are very satisfied‚Äîthough several note it‚Äôs an evolutionary, not revolutionary, upgrade.                                                                    | 2026-01-08 04:49:06.857 | 2026-01-15 04:49:06.857 |
|  2 |         2 | Users praise this 49" super-ultrawide as a highly immersive, productivity- and gaming-focused monitor ‚Äî sharp 5120x1440 resolution, 240Hz/1ms performance, excellent color accuracy (‚âà95% DCI-P3 when calibrated), effective curve, thin bezels and premium build make multitasking and racing/flight sims spectacular. HDR and deep blacks look strong but aren‚Äôt as bright as dedicated HDR displays; G‚ÄëSync and PBP/PiP features add to the experience. Common drawbacks include the need for a powerful GPU and a deep desk, a bulky stand (no height adjust) and no built-in KVM, plus occasional quality-control issues reported (dead pixels, backlight bleed, app-related flicker, and running hot). Overall it‚Äôs a great choice for serious users if you get a well-functioning panel.                                                                                                                               | 2026-01-12 09:34:32.920 | 2026-01-19 09:34:32.921 |
|  3 |         3 | Customers consistently praise the Dyson V15 for powerful suction, effective automatic surface adjustment, and its laser dust-detection and real-time display‚Äîfeatures that many say make cleaning more thorough (especially for pet hair) and reduce the need for professional cleaning. Reviewers also like the versatile attachments, hygienic bin emptying, HEPA filtration, lightweight design, and sturdy wall mount. Common negatives are the high price, long charge time and potential battery degradation (some recommend a spare battery for large homes), occasional top‚Äëheaviness with certain attachments, a tiring trigger power button, and reports of debris scattering on some low‚Äëpile carpets or fine dust sticking in crevices; some users also feel the laser novelty wears off. Overall, buyers find it highly effective and premium, but pricey and not without ergonomic or battery trade-offs.       | 2026-01-13 02:55:07.029 | 2026-01-20 02:55:07.029 |
|  4 |         4 | Customers praise the Switch OLED for its stunning 7-inch OLED display, richer colors and contrast, improved speakers, premium build, sturdier adjustable stand, slightly better battery life, and larger 64GB storage (plus a dock with Ethernet), making it the best Switch for handheld and tabletop play. Common negatives: the internal hardware and performance are unchanged from the original Switch, so frame rates/resolutions don‚Äôt improve and the upgrade feels incremental for dock-focused owners; battery gains are modest, the white Joy‚ÄëCons may show dirt, and some wish a screen protector had been included. Overall, highly recommended for new buyers and handheld players, less compelling for those already owning the original Switch.                                                                                                                                                               | 2026-01-13 03:19:58.067 | 2026-01-20 03:19:58.067 |
|  5 |         5 | Reviewers consistently praise the Instant Pot Duo Plus for its versatility, time-saving performance and value‚Äîpressure cooking tenderizes meats, the saut√©, sous-vide and sterilize/yogurt functions expand cooking options, beans and many dishes finish quickly, large sizes suit meal prep/families, and stainless-steel parts, safety features and easy cleanup earn high marks. Common negatives include a learning curve and limited manual, long heat-up/pressure-release times and loud beeps, occasional inconsistent rice results and inner-pot hot spots that can burn food, a large countertop footprint and a non-backlit display, plus a sealing ring that can retain odors and a keep-warm setting that may overcook delicate dishes. Overall, users find it an indispensable, space- and cost-saving multicooker despite some usability and consistency quirks.                                               | 2026-01-13 03:39:36.612 | 2026-01-20 03:39:36.612 |
+----+-----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------+-------------------------+
5 rows in set (0.004 sec)
-------------------code----------------------
3- to **remove all summaries from the database**.
‚Äúsamureis‚Äù = **summaries** üëç
Your MySQL connection and table listing are correct.

Below are the **correct and safe ways**, explained clearly.

---

## Option 1: Delete all rows (safe, reversible via backup)

-------------------code----------------------
DELETE FROM summaries;
-------------------code----------------------

### What this does

* Removes **all rows** from `summaries`
* Keeps the table structure
* Resets nothing automatically (IDs continue incrementing)

### When to use

* You want to clear data but keep the table
* You care about transactions or triggers
* Safer for development/testing

---

## Option 2: Truncate the table (fast, destructive)

-------------------code----------------------
TRUNCATE TABLE summaries;
-------------------code----------------------

### What this does

* Deletes **all rows instantly**
* Resets `AUTO_INCREMENT` back to 1
* Cannot be rolled back
* Faster than `DELETE`

### When to use

* You want a **hard reset**
* No need to keep IDs
* Usually fine in local/dev environments

---

## Important: Foreign Key Consideration

If `summaries.productId` has a **foreign key constraint**, `TRUNCATE` may fail.

If that happens, use:

-------------------code----------------------
DELETE FROM summaries;
-------------------code----------------------

Or temporarily disable checks (advanced, use carefully):

-------------------code----------------------
SET FOREIGN_KEY_CHECKS = 0;
TRUNCATE TABLE summaries;
SET FOREIGN_KEY_CHECKS = 1;
-------------------code----------------------

---

## Verify the Table Is Empty

After deletion:

-------------------code----------------------
SELECT COUNT(*) FROM summaries;
-------------------code----------------------

Expected result:

-------------------code----------------------
0
-------------------code----------------------

---

## Summary (Quick Choice)

* **Just clear data safely** ‚Üí `DELETE FROM summaries;`
* **Full reset, fastest** ‚Üí `TRUNCATE TABLE summaries;`

*** 5- Choosing the Right Model For the Job ***

## Lecture Summary

**Improving Review Summarization by Switching LLM Models and Refactoring Prompt Handling**

---

## Lesson Context

Previously, the application used **BART**, a model fine-tuned on **CNN / Daily Mail**.
While BART performs well for **news article summarization**, it performs poorly for **customer product reviews** because:

* News summaries focus on factual compression
* Review summaries require opinion aggregation, sentiment extraction, and theme detection

As a result, summaries did not sound natural or useful for product reviews.

---

## 1. Available Solutions

Two possible approaches are identified:

### Option 1: Use a Review-Specific Model

* Search Hugging Face for models fine-tuned on **review summarization**
* Around **40 models** exist
* Example model:

  * ~1B parameters
  * Very low usage (49 downloads)
* Problem:

  * No inference provider available
  * Cannot be called directly via Hugging Face API
  * Requires:

    * Local deployment, or
    * Dedicated hosted endpoint (paid)

This option is not practical for the current setup.

---

### Option 2: Use a General-Purpose LLM

* Choose a strong general model capable of instruction-following
* Similar to GPT-style models
* Open source and available via inference providers

This approach is selected.

---

## 2. Selecting LLaMA

* Model: **LLaMA (Meta)**
* Size: **8 billion parameters**
* Available through Hugging Face inference
* Supports **chat-based completions**

Key difference:

* Instead of `summarization()`, the model uses **chatCompletion**

---

## 3. Switching to `chatCompletion`

### Key API Change

Old approach:

* Call a summarize endpoint directly

New approach:

* Call `chatCompletion`
* Provide:

  * Provider
  * Model
  * Messages array

Messages follow a role-based format:

* `system`: instructions
* `user`: input data (reviews)

---

## 4. Prompt Refactoring

### Previous Design

* Prompt was constructed in the **review service**
* Prompt template included both:

  * Instructions
  * Reviews

### New Design

* Prompt logic moved into the **LLM client**
* Prompt file now contains **instructions only**
* Reviews are passed as user input

This separation improves:

* Clarity
* Reusability
* Responsibility boundaries

-------------------code----------------------
Summarize the following customer reviews into a short paragraph,
highlighting key themes, both positive and negative.
-------------------code----------------------

---

## 5. Using a System Prompt

The first message in `messages` is:

* Role: `system`
* Content: summarize-reviews prompt template

This prompt defines:

* How the model should behave
* What kind of summary to produce
* Tone and structure

The second message is:

* Role: `user`
* Content: actual customer reviews
-------------------code----------------------
import  summarizePrompt  from "../llm/prompts/summarize-reviews.txt";

async summarizeReviews(text: string) {
  const chatCompletion = await inferenceClient.chatCompletion({
    model: "meta-llama/Llama-3.1-8B-Instruct:novita",
    messages: [
      {
        role: "system",
        content: summarizePrompt,
      },
      {
        role: "user",
        content: text,
      },
    ],
  });
  return chatCompletion.choices[0]?.message.content;
},
-------------------code----------------------
---

## 6. Method Renaming for Clarity

Changes made:

* `summarize(text)` ‚Üí `summarizeReviews(reviews)`
* This reflects the real intent of the function
* Avoids misleading generic naming

Clear naming improves:

* Readability
* Maintainability
* API correctness

---

## 7. Extracting the Summary from the Response

The `chatCompletion` response contains:

* `choices` array
* First choice contains:

  * `message`
  * `content`

The summary is extracted from:

-------------------code----------------------
choices[0].message.content
-------------------code----------------------

If the content is missing:

* Return an empty string as a safe fallback

---

## 8. Further Responsibility Cleanup

Because the LLM client now:

* Handles prompt templates
* Handles LLM calls

The review service no longer needs to:

* Construct prompts
* Import prompt templates

This reduces duplication and tight coupling.


---

## 9. Project Structure Improvement

Since prompts are now tightly coupled with LLM logic:

* The `prompts` directory is moved **inside the LLM directory**

This reflects a clear architectural rule:

> Concepts that evolve together should live together

---

## 10. Fixing Broken Imports

After moving the prompts directory:

* Some imports were not automatically updated by the editor
* Errors appeared:

  * Missing prompt files
  * Incorrect relative paths

Fixes required:

* Update imports in:

  * Chat service
  * LLM client module
* Ensure correct path traversal after restructuring

Once fixed, all runtime errors are resolved.
-------------------code----------------------
const parkInfoPath = path.join(
  __dirname,
  "..",
  "llm",
  "prompts",
  "wonderworld.md"
);
-------------------code----------------------
---

## 11. Final Result

After switching to LLaMA and refactoring:

* Summaries sound natural and review-focused
* Example output:

  * Identifies key themes
  * Reflects overall sentiment
  * Reads like a real customer review summary
* Architecture is cleaner:

  * UI ‚Üí Review Service ‚Üí LLM Client ‚Üí Prompt Templates
* Responsibilities are clearly separated

---

## Key Takeaways

* Model choice matters more than size alone
* Fine-tuning domain mismatch leads to poor results
* General-purpose LLMs can outperform specialized models with good prompts
* Prompt logic belongs close to LLM invocation
* Clear naming and structure dramatically improve code quality
* Refactoring is not just cleanup, it enables better results

---


*** 6- Running Models Locally ***
## Lecture Summary

---

## Lesson Motivation

So far, you have learned how to call **Hugging Face models** using the **Inference Client**.
While this approach is convenient, it has an important limitation:

* Prompts and responses are sent **over the network**
* This means:

  * No full privacy
  * External dependency on third-party APIs
  * Potential compliance or data-sensitivity issues

This lesson introduces a solution for **running LLMs locally** with **full privacy**.

---

## What Is Ollama?

**Ollama** is a free, cross-platform tool that allows you to run large language models **locally** on your machine.

Key characteristics:

* Runs entirely on your local system
* No network calls for inference
* Very simple CLI interface
* Conceptually similar to **Docker for LLMs**
* Supports:

  * Ollama‚Äôs own model registry
  * Hugging Face models (locally)

---

## Installing Ollama

1. Visit **ollama.com**
2. Go to the **Downloads** page
3. Download the installer for your operating system
4. Install and run Ollama

After installation:

* Open a terminal
* Run `ollama`
* If you see a list of commands, the installation was successful

---

## Ollama Command Overview

Once installed, Ollama provides several useful commands:

* `ollama pull` ‚Äì download a model
* `ollama push` ‚Äì upload a model
* `ollama list` ‚Äì list downloaded models
* `ollama ps` ‚Äì list running models
* `ollama rm` ‚Äì remove a model

These commands make model lifecycle management straightforward.

---

## Running LLaMA Locally

The **LLaMA** model used in previous lessons is also available in Ollama‚Äôs registry.

Steps:

1. Go to **Explore Models** on ollama.com
2. Search for **llama**
3. Copy the provided command
4. Run it in the terminal

If the model is not already on your machine:

* Ollama will automatically pull it from the registry
* Download time depends on your internet speed

Once complete:

* An interactive shell opens
* You can directly interact with the model

---

## Interacting with the Model

Inside the Ollama shell:

* You can send prompts
* Ask questions
* Summarize text
* Perform general reasoning tasks

Example:

* Ask: ‚ÄúWhat is the capital of France?‚Äù
* Response: ‚ÄúThe capital of France is Paris.‚Äù

This confirms the model is running locally and responding correctly.

---

## Managing Local Models

### Listing Installed Models

Command:

-------------------code----------------------
ollama list
-------------------code----------------------

Displays:

* Model name
* Model ID
* Size on disk
* Last modified time

Example:

* `llama 3.1`
* Shows how large the model is and when it was last updated

---

### Viewing Running Models

Command:

-------------------code----------------------
ollama ps
-------------------code----------------------

Shows:

* Which models are currently running
* Memory usage
* CPU usage

Important note:

* LLMs consume significant system resources
* Monitoring usage is critical on local machines

---

### Removing Models

Because models are large, you should remove them when no longer needed.

Command:

-------------------code----------------------
ollama rm <model-name>
-------------------code----------------------

Example:

-------------------code----------------------
ollama rm llama:3.1
-------------------code----------------------

After removal:

* The model is deleted from disk
* `ollama list` will no longer show it

This helps free up storage and system resources.

---

## Key Advantages of Ollama

* Full data privacy
* No API keys
* No network latency
* Offline usage
* Easy model lifecycle management
* Ideal for:

  * Sensitive data
  * Prototyping
  * Local experimentation
  * Compliance-restricted environments

---

## What Comes Next

The next step is learning how to:

* Run **Hugging Face models locally using Ollama**
* Integrate Ollama into applications instead of remote inference APIs

This bridges the gap between:

* Cloud-based inference
* Fully local AI systems

---

## Optional Follow-Up (If You Want)

I can also:

* Compare Ollama vs Hugging Face inference (architecture and trade-offs)
* Show how to integrate Ollama into a Node.js or backend service
* Explain GPU vs CPU considerations for local LLMs
* Design a production-ready local-LLM setup
* Discuss security and compliance benefits in detail
