1- Audio:
turn audio into text or text into audio.

- open AI use Whisper model 
Whisper is an automatic speech recognition (ASR) system developed by OpenAI.
It's a powerful AI model that can transcribe spoken language from audio into text and even translate multiple languages into English.
Whisper is designed to handle a wide variety of accents, dialects, and background noises, making it versatile for different transcription and translation tasks.

so the Audio API provides two speech to text endpoints
1-transcriptions: refer to the process of converting audio (spoken words) into text. This is typically done using AI models that are trained to "listen" to an audio file and generate a written version of what was said.
2-translations

- Transcriptions
ex: of using the audio file with open ai 
-----------------code----------------------
from openai import OpenAI
client = OpenAI()

audio_file= open("/path/to/file/audio.mp3", "rb")
transcription = client.audio.transcriptions.create(
  model="whisper-1", 
  file=audio_file
)
print(transcription.text)
-------------------code----------------------

result:
-------------------code----------------------
{
  "text": "Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger.
....
}
-------------------code----------------------

- there is as parameter 
ex: the type of the result "text" or "json" etc ...
-------------------code----------------------
from openai import OpenAI
client = OpenAI()

audio_file = open("/path/to/file/speech.mp3", "rb")
transcription = client.audio.transcriptions.create(
  model="whisper-1", 
  file=audio_file, 
  response_format="text"
)
print(transcription.text)
-------------------code----------------------


- Translations
Whisper model support the transcriptions of japanese (Japanese audio to Japanese text also transcribe and translate Japanese audio into English)
but OpenAI's Whisper model does not support English to Japanese so we need to use another model
so when translation Whisper model only support to English

ex: german.mp3 
-------------------code----------------------
from openai import OpenAI
client = OpenAI()

audio_file= open("/path/to/file/german.mp3", "rb")
translation = client.audio.translations.create(
  model="whisper-1", 
  file=audio_file
)
print(translation.text)
-------------------code----------------------

result:
-------------------code----------------------
Hello, my name is Wolfgang and I come from Germany. Where are you heading today?
-------------------code----------------------


- Supported languages:
Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English,
Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada,
Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese,
Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh.

- Timestamps
When you transcribe audio with timestamps, each word or phrase in the transcription is associated with a specific time in the audio.
This is useful for various tasks, such as creating subtitles, analyzing speech in interviews, or syncing audio and video content.
ex:
-------------------code----------------------
[00:05] Hello, how are you?
[00:10] I'm doing well, thank you.
-------------------code----------------------

- Longer inputs
Whisper API only supports files that are less than 25 MB. If you have an audio file that is longer than that, you will need to break it up into chunks of 25 MB's or less or used a compressed audio format. 
To get the best performance, its suggested to avoid breaking the audio up mid-sentence as this may cause some context to be lost.

ex: of handling the splitting the Audio file using the pydub python library (for ruby we can use "streamio-ffmpeg" library) but Pydub is often considered more powerful 
-------------------code----------------------
from pydub import AudioSegment

song = AudioSegment.from_mp3("good_morning.mp3")

# PyDub handles time in milliseconds
ten_minutes = 10 * 60 * 1000

first_10_minutes = song[:ten_minutes]

first_10_minutes.export("good_morning_10.mp3", format="mp3")
-------------------code----------------------

- Prompting
You can use a prompt to improve the quality of the transcripts generated by the Whisper API.
- correcting specific words or acronyms that the model may mis-recognize in the audio:
first the word "GPT" is created, "GPT-3" is consider new word so Whisper may make a mistake an change it "GDP 3".

- preserve the context of a file that was split into segments
When you have a long audio file that you split into smaller parts (segments), 
you can improve the accuracy of the transcription by providing the text from the previous part as context when you ask the model to transcribe the next part.
This way, the model can use the relevant information from the earlier segment to understand the conversation better.

However, the model will only pay attention to the last 224 words (tokens) you provide. Anything before those last 224 tokens will be ignored.

- might skip punctuation
the model might not always include things like commas, periods, question marks, or exclamation points in the final written output.
support by prompt.
"Hello, welcome to my lecture."

- leave out common filler words in the audio. 
"Umm". "Hmm"
ex:
"Umm, let me think like, hmm... Okay, here's what I'm, like, thinking."


As we explored in the prompting section, one of the most common challenges faced when using Whisper is the model often does not recognize uncommon words or acronyms.
To address this, we have highlighted different techniques which improve the reliability of Whisper in these cases:

- Using the prompt parameter:
The first method involves using the optional prompt parameter to pass a dictionary of the correct spellings.

Since it wasn't trained using "instruction-following" techniques, Whisper operates more like a base GPT model.
It's important to keep in mind that Whisper only considers the first 244 tokens of the prompt.
-------------------code----------------------
from openai import OpenAI
client = OpenAI()

audio_file = open("/path/to/file/speech.mp3", "rb")
transcription = client.audio.transcriptions.create(
  model="whisper-1", 
  file=audio_file, 
  response_format="text",
  prompt="ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T."
)
print(transcription.text)
-------------------code----------------------

- Post-processing with GPT-4
The second method involves a post-processing step using GPT-4 or GPT-3.5-Turbo.
providing instructions for GPT-4 through the system_prompt variable. Similar to what we did with the prompt parameter earlier, we can define our company and product names.
-------------------code----------------------
system_prompt = "You are a helpful assistant for the company ZyntriQix. Your task is to correct any spelling discrepancies in the transcribed text. Make sure that the names of the following products are spelled correctly: ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T. Only add necessary punctuation such as periods, commas, and capitalization, and use only the context provided."

def generate_corrected_transcript(temperature, system_prompt, audio_file):
    response = client.chat.completions.create(
        model="gpt-4o",
        temperature=temperature,
        messages=[
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": transcribe(audio_file, "")
            }
        ]
    )
    return completion.choices[0].message.content

corrected_text = generate_corrected_transcript(0, system_prompt, fake_company_filepath)
-------------------code----------------------
